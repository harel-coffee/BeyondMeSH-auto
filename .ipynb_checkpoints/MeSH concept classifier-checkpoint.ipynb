{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeSH concept classifier \n",
    "Based in this example [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data set\n",
    "1. Create CSV file with *articles per class* based on C<sup>t</sup> occurence in articles for t. (Java \"SaveInMongo\" and export with MongoChef)\n",
    "2. Create CSV file with articles for t with harvested fields e.g. *abstract text* (Java \"EntrezHarvester\" and export with MongoChef)\n",
    "3. Create \"distantly supervised\" *data sets* (jupyter notebook \"MeSH concept datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# case study\n",
    "cs = 'DMD';\n",
    "# cs = 'AD';\n",
    "# cs = 'LC';\n",
    "input_dir = 'D:/05 MeSHToUMLS/DSTDS/ArticlesPerClass/' + cs;\n",
    "test_dir = 'D:/05 MeSHToUMLS/DSTDS/ArticlesPerClass/' + cs + '_noClass';\n",
    "pred_dir = 'D:/05 MeSHToUMLS/Predictions/' + cs + '/20180427b'\n",
    "# os.makedirs(pred_dir)\n",
    "Manual_evaluation_pmids = 'D:/05 MeSHToUMLS/ManualDS/DMD/v2/BioASQ_format/Datasets/DMD_pmids.txt';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset \n",
    "Into a \"Bunch\" object, directly from text files organized in one folder per class ([ref](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "train_dataset = sklearn.datasets.load_files(input_dir,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Bunch fields and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "['C0013264', 'C0917713', 'C3542021']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.keys())\n",
    "print(train_dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert dataset to multi-label \n",
    "Merge articles included in more than one folders into one with both labels\n",
    "<br> and replace \"filenames\" by just \"pmids\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3179\n",
      "3179\n",
      "3179\n"
     ]
    }
   ],
   "source": [
    "#Ittereate the whole dataset\n",
    "ml_filenames = []\n",
    "ml_data = []\n",
    "# alternative representation of multiple classes, used to call MultiLabelBinarizer\n",
    "ml_targets_alt = []\n",
    "\n",
    "# for all articles in the dataset, including \"duplicate\" articles apearing in more than one topic folders\n",
    "for data, filename, target in zip(train_dataset.data, train_dataset.filenames, train_dataset.target):\n",
    "    # get the pmid of the article\n",
    "    words = filename.split(\"\\\\\")\n",
    "#     print('%r <= %s' % ( words[-1].replace(\".txt\",\"\"),target)) \n",
    "    pmid = filename[53:-4]\n",
    "    # add the article in ml_filenames list\n",
    "    if ml_filenames.count(pmid) == 0:\n",
    "        ml_filenames.append(pmid)\n",
    "        # ml_targets.append([0]*len(train_dataset.target_names)) \n",
    "        ml_targets_alt.append([]) \n",
    "        # add corresponding data in ml_data\n",
    "        ml_data.insert(ml_filenames.index(pmid),data)\n",
    "    # Updated corresponding labels in ml_targets\n",
    "#     ml_targets[ml_filenames.index(pmid)][target] = 1\n",
    "    ml_targets_alt[ml_filenames.index(pmid)].append(target)\n",
    "\n",
    "#     print(ml_targets)\n",
    "# print(ml_targets_alt)\n",
    "\n",
    "# update DMD_train dataset\n",
    "train_dataset.data = ml_data\n",
    "train_dataset.filenames = ml_filenames\n",
    "train_dataset.target = ml_targets_alt\n",
    "\n",
    "# print dataset number of elements\n",
    "print(len(train_dataset.data))\n",
    "print(len(train_dataset.filenames))\n",
    "print(len(train_dataset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usie MultiLabelBinarizer to create binary 2d matrix with one column per class as done here : [ref](http://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = MultiLabelBinarizer().fit_transform(ml_targets_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize article text \n",
    "Produce a sparse representation of the counts in a scipy.sparse.csr_matrix ([ref](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "# Learn the vocabulary dictionary (fit) and return term-document matrix (transform).\n",
    "train_counts = count_vect.fit_transform(train_dataset.data)\n",
    "# Print sample No (articles) and feature No (tokens)\n",
    "train_counts.shape\n",
    "# print (sample no: article, feature no: word id)  number of occurences\n",
    "# print(train_counts)\n",
    "\n",
    "# get inverted vocabulary\n",
    "inv_vocabulary = {v: k for k, v in count_vect.vocabulary_.items()}\n",
    "# print(inv_vocabulary)\n",
    "\n",
    "# Create a list with the names of the features\n",
    "feature_names = []\n",
    "for i in range(len(count_vect.vocabulary_)) :\n",
    "    feature_names.insert(i,inv_vocabulary[i])\n",
    "# print(feature_names)\n",
    "# print(len(feature_names))\n",
    "# feature_names = list(count_vect.vocabulary_.keys())\n",
    "# feature_names = list(count_vect.vocabulary_.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print token count details \n",
    "Print 'term string' : term id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print stop words excluded from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.stop_words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print (article id, term id)  term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article  27011057  contains  0  times the term :  becker\n"
     ]
    }
   ],
   "source": [
    "# Print term frequency and term for sepcific pair of article and termid\n",
    "article_id = 0;\n",
    "# term_id = 12845\n",
    "term_id = 2932\n",
    "print(\"Article \",train_dataset.filenames[article_id],\" contains \", train_counts[article_id,term_id], \" times the term : \", inv_vocabulary[term_id],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tf-idf features\n",
    "Get \"term frequency\" times \"inverse document frequency\" based on term occurrences calculated ([ref](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html))\n",
    "1. Learn the idf vector \n",
    "2. Transform the idf count matrix to a tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3179, 19572)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# Fit to data, then transform it to a tf-idf matrix.\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "# Print sample No (articles) and feature No (tokens)\n",
    "train_tfidf.shape\n",
    "# print(train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print (article id, term id)   tf-idf feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st alternative  \n",
    "A ***DecisionTreeClassifier*** ([ref](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))\n",
    "<br/> Inherently multiclass and multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "# train the classifier so that the given feature fectors to fit the given target classes\n",
    "clf = DecisionTreeClassifier().fit(train_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of the classifier\n",
    "Visualize the Tree based on this example ([ref](https://github.com/MSc-in-Data-Science/class_material/blob/master/semester_1/Machine_Learning/Lecture_3-DecisionTrees/Lecture_3-DecisionTrees.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\Tasks\\0 All Projects\\MeSHtoUMLS\\venv\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(engine, format, data, quiet)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             startupinfo=STARTUPINFO)\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    996\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    998\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\Tasks\\0 All Projects\\MeSHtoUMLS\\venv\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tasks\\0 All Projects\\MeSHtoUMLS\\venv\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tasks\\0 All Projects\\MeSHtoUMLS\\venv\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tasks\\0 All Projects\\MeSHtoUMLS\\venv\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(engine, format, data, quiet)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.files.Source at 0xe1e1750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=feature_names) \n",
    "# dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.render(pred_dir + '/Dtree', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd alternative  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *** OneVsRest Classifier *** ([ref](http://scikit-learn.org/stable/modules/multiclass.html#multiclass-learning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training dataset to ndarray from crs matrix ([ref](https://stackoverflow.com/questions/31228303/scikit-learns-pipeline-error-with-multilabel-classification-a-sparse-matrix-w?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3179, 19572)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf.shape)\n",
    "X_train_tfidf = train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d0b2b116abce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf =  OneVsRestClassifier(LinearSVC())\n",
    "clf.fit(X_train_tfidf,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classifier properties intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"intercept :\\t\",clf.intercept_)\n",
    "# print(clf.coef_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top and bottom feature coefficients per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top elements to be printed\n",
    "number_of_elements = 10\n",
    "    \n",
    "for class_index in range(len(train_dataset.target_names)):\n",
    "    print(\"\\nFeature coefficients for class : \",train_dataset.target_names[class_index])\n",
    "    feature_coef_ = {}\n",
    "    for feat, coef in zip(feature_names,clf.coef_[class_index]):\n",
    "    #     print(coef,\"\\t\",feat)\n",
    "        feature_coef_[feat]=coef\n",
    "\n",
    "    # print(feature_coef_) \n",
    "\n",
    "    # sort by coef size\n",
    "    import operator\n",
    "    sorted_feature_coef_ = sorted(feature_coef_.items(), key=operator.itemgetter(1),reverse=True)    \n",
    "    # print(sorted_feature_coef_)  \n",
    "    print(\"\\n*** top \",number_of_elements,\" ***\\n\")\n",
    "    for feat_coef in sorted_feature_coef_[:number_of_elements]:\n",
    "        print(feat_coef[1],\"\\t\",feat_coef[0])\n",
    "\n",
    "    sorted_feature_coef_ = sorted(feature_coef_.items(), key=operator.itemgetter(1),reverse=False)        \n",
    "    print(\"\\n*** bottom \",number_of_elements,\" ***\\n\")\n",
    "    for feat_coef in sorted_feature_coef_[:number_of_elements]:\n",
    "        print(feat_coef[1],\"\\t\",feat_coef[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform crossvalidation ([ref](http://scikit-learn.org/stable/modules/cross_validation.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# By score it means accuracy\n",
    "scores = cross_val_score(clf, train_tfidf, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the classifier ([ref](https://github.com/MSc-in-Data-Science/class_material/blob/master/semester_1/Machine_Learning/Lecture_9-SVM/Support%20Vector%20Machines.ipynb)) (I have to select two valid features to create a visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# X = train_tfidf[:, :2] \n",
    "# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# h = (x_max / x_min)/100\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#  np.arange(y_min, y_max, h))\n",
    "# plt.subplot(1, 1, 1)\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.contourf(xx, yy, Z,cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='black',cmap=plt.cm.RdYlBu)\n",
    "# plt.xlabel('Sepal length')\n",
    "# plt.ylabel('Sepal width')\n",
    "# plt.xlim(xx.min(), xx.max())\n",
    "# plt.title('SVC with ' + kernel + ' kernel')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection ([ref](https://github.com/MSc-in-Data-Science/class_material/blob/master/semester_1/Machine_Learning/Lecture_11-FeatureSelection/FeatureSelection.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X = train_tfidf\n",
    "Y = y\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "numpy.set_printoptions(precision=3)\n",
    "# print(\"Fit scores:\")\n",
    "# print(fit.scores_)\n",
    "# print(feature_names)\n",
    "feature_score_univariate = {}\n",
    "for xi,yi in zip(feature_names,fit.scores_):\n",
    "    feature_score_univariate[xi]=yi\n",
    "#     print(\"Feature: \"+xi+\"\\t score: \",yi)\n",
    "# print(feature_score_univariate)\n",
    "\n",
    "# sort the dict by values with key=operator.itemgetter(0) or by keys with key=operator.itemgetter(1). Descenting with reverse=True\n",
    "import operator\n",
    "sorted_feature_score_univariate = sorted(feature_score_univariate.items(), key=operator.itemgetter(1),reverse=True)\n",
    "# print(sorted_feature_score_univariate[:50])\n",
    "\n",
    "print(\"\\n*** top 50 ***\\n\")\n",
    "for weight in sorted_feature_score_univariate[:50]:\n",
    "    print(weight[1],\"\\t\",weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print bottom feature scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dict by values with key=operator.itemgetter(0) or by keys with key=operator.itemgetter(1). Descenting with reverse=True\n",
    "import operator\n",
    "sorted_feature_score_univariate = sorted(feature_score_univariate.items(), key=operator.itemgetter(1),reverse=False)\n",
    "# print(sorted_feature_score_univariate[:50])\n",
    "\n",
    "print(\"\\n*** Bottom 50 ***\\n\")\n",
    "for weight in sorted_feature_score_univariate[:50]:\n",
    "    print(weight[1],\"\\t\",weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Feature Importance \n",
    "Bagged decision trees like Random Forest and Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd alternative  \n",
    "A decission forest Classifier (find an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test dataset (use \"rest articles\" for test here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = sklearn.datasets.load_files(test_dir,encoding=\"utf8\")\n",
    "#Ittereate the whole dataset\n",
    "ml_filenames = []\n",
    "ml_data = []\n",
    "\n",
    "# for all articles in the dataset, including \"duplicate\" articles apearing in more than one topic folders\n",
    "for filename, data in zip(test_dataset.filenames,test_dataset.data):\n",
    "    # get the pmid of the article\n",
    "#     print(filename)\n",
    "    words = filename.split(\"\\\\\")\n",
    "#     print('%r <= %s' % ( words[-1].replace(\".txt\",\"\"),target)) \n",
    "    pmid = filename[57:-4]\n",
    "#     print(pmid)\n",
    "    ml_filenames.append(pmid)\n",
    "    ml_data.insert(ml_filenames.index(pmid),data)\n",
    "    \n",
    "# update DMD_test dataset\n",
    "\n",
    "test_dataset.data = ml_data\n",
    "test_dataset.filenames = ml_filenames\n",
    "\n",
    "# print dataset number of elements\n",
    "print(len(test_dataset.data))\n",
    "# print(len(train_dataset.filenames))\n",
    "# print(len(train_dataset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = test_dataset.data\n",
    "# docs_new = [#DMD C0013264\n",
    "#             \"Enhanced expression of recombinant dystrophin following intramuscular injection of Epstein-Barr virus (EBV)-based mini-chromosome vectors in mdx mice.Gene transfer by direct intramuscular injection of naked plasmid DNA has been shown to be a safe, simple but relatively inefficient method for gene delivery in vivo. Eukaryotic plasmid expression vectors incorporating the Epstein-Barr virus (EBV) origin of replication (oriP) and EBNA1 gene have been shown to act as autonomous episomally replicating gene transfer vectors which additionally provide nuclear matrix retention functions. Prolonged expression of a LacZ reporter gene and recombinant human dystrophin was shown using EBV-based plasmid vectors transfected into C2C12 mouse myoblast and myotube cultures. Intramuscular injection of EBV-based dystrophin expression plasmids into nude/mdx mice resulted in significant enhancement in the number of muscle fibres expressing recombinant dystrophin compared with a conventional vector. This effect was observed for over 10 weeks after a single administration. These results indicate the potential advantage of EBV-based expression vectors for focal plasmid-mediated gene augmentation therapy in Duchenne muscular dystrophy (DMD) and a range of other gene therapeutic applications.\",\n",
    "#             #BMD C0917713\n",
    "#             \"Cardiomyopathy in duchenne, becker, and sarcoglycanopathies: a role for coronary dysfunction?Dilated cardiomyopathy is a feature of Duchenne and Becker muscular dystrophies and occasionally of sarcoglycanopathies. Its pathogenesis is unknown. Patients with myotonic dystrophy have an impairment of coronary smooth muscle and this could contribute to their cardiomyopathy. We used positron emission tomography (PET) to study myocardial blood flow and coronary vasodilator reserve at baseline and during hyperemia in 7 Duchenne, 8 Becker, and 5 sarcoglycanopathy patients. The study was normal in all Becker patients. In contrast, baseline myocardial blood flow was increased and coronary vasodilator reserve blunted in Duchenne and sarcoglycanopathy patients despite normal hyperemic myocardial blood flow. The reduction of coronary vasodilator reserve was due to an increased baseline myocardial blood flow. In Duchenne dystrophy, but not in sarcoglycanopathies, correction for cardiac workload normalized the coronary vasodilator reserve. In the latter patients, abnormal baseline myocardial blood flow could be due to vascular smooth muscle dysfunction.\",\n",
    "#             #BDMD C3542021\n",
    "#             \"[Application of PCR technique in genetic diagnosis of Duchenne/Becker muscular dystrophy].OBJECTIVE: To study the application of PCR technique in genetic detection of Duchenne/Becker muscular dystrophy (DMD/BMD).METHODS: A multiple PCR system is established according to the multiple sites of DMD/BMD exon deletion. Under different PCR conditions, multiple exon deletions, single-strand conformation polymorphism, allopolyploid, chain labeling, restriction fragment length polymorphism and microsatellite phenomenon were examined in 23 DMD/BMD patients and 57 suspected carriers of these genes.RESULTS: Fourteen of the 23 DMD/BMD patients were identified as having gene deletion, with another 2 carried gene duplicates. Forty female relatives of these 23 DMD/BMD patients were diagnosed as carriers of the genes.CONCLUSION: This PCR system can be applied in detecting gene mutation of DMD/BMD, screening the carriers and in appropriate genealogical analysis of the patients with DMD/BMD.\",\n",
    "# ]\n",
    "test_counts = count_vect.transform(docs_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_counts)\n",
    "\n",
    "# Get the predictions\n",
    "predicted_test = clf.predict(test_tfidf)\n",
    "\n",
    "# Print the predictions\n",
    "for doc, categories, pmid in zip(docs_test, predicted_test,test_dataset.filenames):\n",
    "    label = \"\"\n",
    "    for category in range(0, len(categories)):\n",
    "        if categories[category] == 1:\n",
    "            label += train_dataset.target_names[category] + \" \" \n",
    "    print('%r <= %s : %s...' % ( label,pmid,doc[:55]))\n",
    "\n",
    "# Write the predictions in a file    \n",
    "file = open(pred_dir + \"/prediction_rest.txt\", \"w\")\n",
    "predictionMap = {}\n",
    "for pmid, prediction in zip(test_dataset.filenames, predicted_test):\n",
    "    file.write(pmid + \" \" )\n",
    "    predictionMap[pmid] = prediction\n",
    "    for category in range(0, len(prediction)):\n",
    "        if(prediction[category]):\n",
    "            file.write(train_dataset.target_names[category]+ \" \" )\n",
    "    file.write(\"\\n\")     \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_counts = count_vect.transform(train_dataset.data)\n",
    "new_tfidf = tfidf_transformer.transform(new_counts)\n",
    "\n",
    "predicted = clf.predict(new_tfidf)\n",
    "np.mean(predicted == train_dataset.target)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results into a file\n",
    "In a simple format : \"pmid class1 class2 ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(pred_dir + \"/prediction.txt\", \"w\")\n",
    "predictionMap = {}\n",
    "for pmid, prediction in zip(train_dataset.filenames, predicted):\n",
    "    file.write(pmid + \" \" )\n",
    "    predictionMap[pmid] = prediction\n",
    "    for category in range(0, len(prediction)):\n",
    "        if(prediction[category]):\n",
    "            file.write(train_dataset.target_names[category]+ \" \" )\n",
    "    file.write(\"\\n\") \n",
    "for pmid, prediction in zip(test_dataset.filenames, predicted_test):\n",
    "    file.write(pmid + \" \" )\n",
    "    predictionMap[pmid] = prediction\n",
    "    for category in range(0, len(prediction)):\n",
    "        if(prediction[category]):\n",
    "            file.write(train_dataset.target_names[category]+ \" \" )\n",
    "    file.write(\"\\n\")     \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select articles\n",
    "Create a file with predictions for selected articles (from a specific dataset e.g. the manually annotated)\n",
    "In the simple BioASQ format \" class1 class2 ...\" for a given order of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(Manual_evaluation_pmids, \"r\")\n",
    "file3 = open(pred_dir + \"/prediction_in_pmids.txt\", \"w\")\n",
    "\n",
    "pmids = file2.readlines()\n",
    "# print(pmids)\n",
    "for pmid in pmids:\n",
    "    pmid = pmid.replace(\"\\n\",\"\")\n",
    "    pmid = pmid.replace(\"\\'\",\"\")\n",
    "    prediction = predictionMap[pmid]\n",
    "    for category in range(0, len(prediction)):\n",
    "            if(prediction[category]):\n",
    "#                 print(train_dataset.target_names[category]+ \" \")\n",
    "                file3.write(train_dataset.target_names[category]+ \" \" )\n",
    "    file3.write(\"\\n\" )\n",
    "\n",
    "file2.close()\n",
    "file3.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
